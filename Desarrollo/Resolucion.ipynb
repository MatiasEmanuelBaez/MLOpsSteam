{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "### TRABAJAMOS SOBRE EL ARCHIVO DE JUEGOS ###\n",
    "#############################################\n",
    "\n",
    "datos=[]            # Definimos/limpiamos una lista que servira como nexo entre los datos y el dataframe.\n",
    "with open('output_steam_games.json') as archivo:            # Abrimos el archivo json.\n",
    "    for linea in archivo:\n",
    "        datos.append(json.loads(linea))         # Agregamos cada linea del json a la lista ya creada.\n",
    "\n",
    "dfGames = pd.DataFrame(datos)          # Creamos un dataframe con los datos de la lista.\n",
    "dfGames = dfGames.dropna(axis=0, how='all')           # Eliminamos del dataframe las filas en las que todos sus campos son nulos.\n",
    "\n",
    "dfGames = dfGames.drop(['publisher', 'title', 'url', 'specs', 'price', 'reviews_url', 'price', 'early_access', 'developer'], axis=1)          # Eliminamos las columnos que no seran de ayuda en el analisis.\n",
    "\n",
    "dfGames = dfGames.dropna(subset=['release_date'])        # Eliminanos los datos nan de la columna de fechas.\n",
    "\n",
    "for fila in dfGames['release_date']:\n",
    "    try:\n",
    "        dfGames.loc[dfGames['release_date'] == fila, 'nueva_fecha'] = datetime.strptime(fila, \"%Y-%m-%d\").year         # Guardamos en una nueva columna, las fechas con formato valido.\n",
    "    except ValueError:\n",
    "       dfGames.loc[dfGames['release_date'] == fila, 'nueva_fecha'] = None         # Si la fecha no es valida, guardamos un nan en su lugar.\n",
    "       \n",
    "\n",
    "dfGames = dfGames.drop(['release_date'], axis=1)            # Eliminamos la columna que ya no sera de ayuda.\n",
    "dfGames = dfGames.rename(columns={'nueva_fecha': 'release_date'})         # Renombramos la nueva columna con el nombre que ya nos hemos familiarizado.\n",
    "dfGames = dfGames.dropna(subset=['release_date'])        #Eliminanos los datos nan de la columna de fechas.\n",
    "dfGames = dfGames.explode('genres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "### TRABAJAMOS SOBRE EL ARCHIVO DE REVIEWS ###\n",
    "##############################################\n",
    "\n",
    "datos=[]            # Definimos/limpiamos una lista que servira como nexo entre los datos y el dataframe.\n",
    "\n",
    "with open('australian_user_reviews.json') as archivo:           # Abrimos el archivo json.\n",
    "    for lineas in archivo:\n",
    "        linea = ast.literal_eval(lineas)            # Ayuda a manejar el \"json\" sin que el formato traiga problemas.\n",
    "        datos.append(linea)         # Agregamos cada linea del \"json\" a la lista ya creada.\n",
    "\n",
    "dfReviews = pd.DataFrame(datos)         # Creamos un dataframe con los datos de la lista.\n",
    "dfReviews = dfReviews.drop(['user_url'], axis=1)            # Eliminamos las columnos que no seran de ayuda en el analisis.\n",
    "\n",
    "\n",
    "dfReviews = dfReviews.drop_duplicates('user_id')            # Eliminamos los usuarios que esten duplicados.\n",
    "dfReviews = dfReviews.explode('reviews')          # Se desanidan los reviews, trasladando cada una a una fila diferente.\n",
    "\n",
    "dfReviews.dropna()          # Eliminamos los registros nan.\n",
    "dfReviews = dfReviews.dropna(subset=['reviews'])            # Eliminamos las filas que no tengan reviews.\n",
    "\n",
    "for key in dfReviews['reviews'].iloc[0].keys():         \n",
    "    dfReviews[key] = dfReviews['reviews'].apply(lambda x: x[key])         # Toma cada \"llave/valor\" correspondiente a las reviews y arma columnas aparte para tener los datos ordenados y accesibles.\n",
    "    \n",
    "dfReviews = dfReviews.drop(['reviews', 'last_edited'], axis=1)         # Eliminamos la columna REVIEWS, la cual ya fue desanidada y otras que no utilizaremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "### TRABAJAMOS SOBRE EL ARCHIVO DE ITEMS ###\n",
    "############################################\n",
    "\n",
    "datos=[]            # Definimos/limpiamos una lista que servira como nexo entre los datos y el dataframe.\n",
    "\n",
    "with open('australian_users_items.json') as archivo:            # Abrimos el archivo json.\n",
    "    for lineas in archivo:\n",
    "        linea = ast.literal_eval(lineas)         # Ayuda a manejar el \"json\" sin que el formato traiga problemas.\n",
    "        datos.append(linea)         # Agregamos cada linea del \"json\" a la lista ya creada.\n",
    "\n",
    "dfItems = pd.DataFrame(datos)         # Creamos un dataframe con los datos de la lista.\n",
    "dfItems = dfItems.drop(['user_url', 'steam_id'], axis=1)            # Eliminamos las columnos que no seran de ayuda en el analisis.\n",
    "\n",
    "\n",
    "dfItems = dfItems.drop_duplicates('user_id')            # Eliminamos los usuarios que esten duplicados.\n",
    "dfItems = dfItems.drop(dfItems[(dfItems['items_count'] == 0)].index)            # Eliminamos los usuarios que no tengan juegos.\n",
    "dfItems = dfItems.explode('items')          # Se desanidan los juegos, trasladando cada uno a una fila diferente.\n",
    "\n",
    "for key in dfItems['items'].iloc[0].keys():         \n",
    "    dfItems[key] = dfItems['items'].apply(lambda x: x[key])         # Toma cada \"llave/valor\" correspondiente a los juegos y arma columnas aparte para tener los datos ordenados y accesibles.\n",
    "    \n",
    "dfItems = dfItems.drop(['items', 'items_count', 'playtime_2weeks'], axis=1)         # Eliminamos la columna ITEMS, la cual ya fue desanidada y otras que no utilizaremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "### PRIMER CONSULTA ###\n",
    "#######################\n",
    "\n",
    "dfAux = dfItems.groupby(by='item_id').playtime_forever.sum()            # Agrupamos los juegos, obteniendo las horas totales de juego.\n",
    "dfResultadoU = pd.merge(dfGames, dfAux, how= 'inner', left_on='id', right_on='item_id')         # Agregamos al dataframe JUEGOS, el total de horas jugadas en c/u.\n",
    "dfResultadoU = dfResultadoU.dropna(subset=['genres'])        #Eliminanos los datos nan de la columna de fechas.\n",
    "\n",
    "dfResultadoU = dfResultadoU.groupby(['genres' , 'release_date'])['playtime_forever'].sum().reset_index()           # Agrupamos los datos por genero y año, sumando las horas.\n",
    "dfResultadoU['max'] = dfResultadoU.groupby('genres')['playtime_forever'].transform(max)           # Detectamos la mayor cantidad de horas y las agregamos a una columna auxiliar.\n",
    "dfResultadoU = dfResultadoU[dfResultadoU['playtime_forever'] == dfResultadoU['max']]         # Filtramos el dataframe para que traiga solo los generos y el año con la mayor cantidad de horas.\n",
    "\n",
    "### EXPORTAMOS A .CSV EL DATAFRAME ###\n",
    "os.makedirs('Resultados', exist_ok=True)\n",
    "dfResultadoU.to_csv('Resultados/PConsulta.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "### SEGUNDA CONSULTA ###\n",
    "########################\n",
    "\n",
    "dfResultadoD = pd.merge(dfGames, dfReviews, how = 'inner', left_on='id', right_on='item_id')            # Creamos un nuevo dataframe, agrupando los juegos y las reviews.\n",
    "dfResultadoD = pd.merge(dfResultadoD, dfItems, how='inner', left_on=['user_id', 'item_id'], right_on=['user_id', 'item_id'])            # Agregamos al mismo dataframe, el tiempo juegado.\n",
    "dfResultadoD = dfResultadoD.drop(['tags', 'release_date', 'funny', 'item_id', 'helpful', 'recommend', 'review', 'item_name'], axis= 1)          # Eliminamos las columnas que no seras utilizadas.\n",
    "\n",
    "for fila in dfResultadoD['posted']:\n",
    "    try:\n",
    "        dfResultadoD.loc[dfResultadoD['posted'] == fila, 'nueva_fecha'] = datetime.strptime(fila, 'Posted %B %d, %Y.').year         # Guardamos en una nueva columna, las fechas con formato valido (poseen anio).\n",
    "    except ValueError:\n",
    "       dfResultadoD.loc[dfResultadoD['posted'] == fila, 'nueva_fecha'] = None         # Si la fecha no es valida (no posee anio), guardamos un nan en su lugar.\n",
    "\n",
    "dfResultadoD = dfResultadoD.dropna(subset=['nueva_fecha'])        #Eliminanos los datos nan de la columna de fechas.\n",
    "dfResultadoD = dfResultadoD.groupby(['user_id', 'nueva_fecha', 'genres'])['playtime_forever'].sum().reset_index()           # Agrupamos los datos por usuario, anio y genero, sumando las horas.\n",
    "dfResultadoD['max'] = dfResultadoD.groupby(['user_id', 'genres'])['playtime_forever'].transform(sum)           # Sumamos las horas totales por genero de cada jugador y las agregamos a una columna auxiliar.\n",
    "\n",
    "### EXPORTAMOS A .CSV EL DATAFRAME ###\n",
    "os.makedirs('Resultados', exist_ok=True)\n",
    "dfResultadoD.to_csv('Resultados/SConsulta.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
